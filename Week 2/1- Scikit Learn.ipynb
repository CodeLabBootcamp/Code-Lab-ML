{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "colab": {
   "name": "Scikit Learn.ipynb",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLHAiAzQEqU2"
   },
   "source": [
    "# Scikit-Learn\n",
    "\n",
    "{{ badge }}\n",
    "\n",
    "[Scikit-learn](http://scikit-learn.org/stable/) is a python-based machine learning library providing implementations of a great many algorithms for supervised and unsupervised learning. In large part, it builds upon the cabilities of NumPy, SciPy, matplotlib, and Pandas.\n",
    "\n",
    "In the context of supervised learning, the primary objects scikit-learn defines are called **estimators**. Each of these defines a `fit` method, which develops a model from provided training data, and a `predict` method, which uses the model to map a new instance to a suitable target value. Scikit-learn also defines multiple utilities for partitioning and manipulating data sets as well as evaluating models.\n",
    "\n",
    "Below, we cover some of the basic steps needed to create a model in scikit-learn.  These notes are based on material appearing in the *scikit-learn tutorials*.\n",
    "\n",
    "*  http://scikit-learn.org/stable/tutorial/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvzKfYgXEqVC"
   },
   "source": [
    "## Datasets\n",
    "\n",
    "Scikit-learn comes bundled with several pre-defined (typically small) `datasets` that users can explore.\n",
    "\n",
    "    load_boston()\tLoad and return the boston house-prices dataset (regression).\n",
    "    load_iris()\tLoad and return the iris dataset (classification).\n",
    "    load_diabetes()\tLoad and return the diabetes dataset (regression).\n",
    "    load_digits()\tLoad and return the digits dataset (classification).\n",
    "    load_linnerud()\tLoad and return the linnerud dataset (multivariate regression).\n",
    "    load_wine()\tLoad and return the wine dataset (classification).\n",
    "    load_breast_cancer()\tLoad and return the breast cancer wisconsin dataset (classification).\n",
    "\n",
    "The iris dataset is loaded below, and a description of it is printed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tPAFLH3AEqVD"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# using 'from * import ...' allows as to import submodules directly\n",
    "from sklearn import datasets, model_selection, linear_model , metrics\n",
    "# alternatively, we can import the whole package as such\n",
    "import sklearn"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FARFibuOuHaw",
    "outputId": "35fb12a2-6b29-4d07-f709-0feef3c3fb7c"
   },
   "source": [
    "iris_dataset = datasets.load_iris() # sklearn.datasets.load_iris() works exactly the same\n",
    "\n",
    "print(iris_dataset.DESCR)"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idq1o-aou6WR"
   },
   "source": [
    "We can also use `iris_dataset.data` and `iris_dataset.targets` to create or x & y (inputs & outputs) pairs that will be used for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "JbXkcYblu6DT",
    "outputId": "1b3488c0-9606-4b87-c0ed-0d48dabd3764"
   },
   "source": [
    "x = pd.DataFrame(iris_dataset.data, columns=iris_dataset.feature_names)\n",
    "y = pd.DataFrame(iris_dataset.target, columns=['Labels'])\n",
    "\n",
    "x"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n0                  5.1               3.5                1.4               0.2\n1                  4.9               3.0                1.4               0.2\n2                  4.7               3.2                1.3               0.2\n3                  4.6               3.1                1.5               0.2\n4                  5.0               3.6                1.4               0.2\n..                 ...               ...                ...               ...\n145                6.7               3.0                5.2               2.3\n146                6.3               2.5                5.0               1.9\n147                6.5               3.0                5.2               2.0\n148                6.2               3.4                5.4               2.3\n149                5.9               3.0                5.1               1.8\n\n[150 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>6.3</td>\n      <td>2.5</td>\n      <td>5.0</td>\n      <td>1.9</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>6.2</td>\n      <td>3.4</td>\n      <td>5.4</td>\n      <td>2.3</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>5.9</td>\n      <td>3.0</td>\n      <td>5.1</td>\n      <td>1.8</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows Ã— 4 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5S3W8bkDuvH4"
   },
   "source": [
    "Alternatively, can  load a dataset into x & y directly (i.e. into input/output pairs) by setting the `return_X_y` parameter to `True`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IqmQMvHbuuZq",
    "outputId": "6895dccd-8d22-4690-9b8a-53b66ab52977"
   },
   "source": [
    "x,y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "x.shape, y.shape"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "((150, 4), (150,))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVzD_4buvw8d"
   },
   "source": [
    "## Train/Test Split\n",
    "\n",
    "In order to validate that our model can generalize to data that it wasn't trained on, it's necessary to create a sperate **testing dataset** that will not be used in training.\n",
    "\n",
    "Within the `model_selection` submodule of Scikit Learn, there's the `train_test_split` that we can use to automatically split the data into training and testing pairs.\n",
    "\n",
    "Here's an explanation of the different parameters taken directly from the function's docstring\n",
    "\n",
    "#### **Parameters**\n",
    "\n",
    "***arrays** : sequence of indexables with same length / shape[0]\n",
    "    Allowed inputs are lists, numpy arrays, scipy-sparse\n",
    "    matrices or pandas dataframes.\n",
    "\n",
    "**test_size** : float, int or None, optional (default=None)\n",
    "    If float, should be between 0.0 and 1.0 and represent the proportion\n",
    "    of the dataset to include in the test split. If int, represents the\n",
    "    absolute number of test samples. If None, the value is set to the\n",
    "    complement of the train size. If train_size is also None, it will\n",
    "    be set to 0.25.\n",
    "\n",
    "**train_size** : float, int, or None, (default=None)\n",
    "    If float, should be between 0.0 and 1.0 and represent the\n",
    "    proportion of the dataset to include in the train split. If\n",
    "    int, represents the absolute number of train samples. If None,\n",
    "    the value is automatically set to the complement of the test size.\n",
    "\n",
    "**random_state** : int, RandomState instance or None, optional (default=None)\n",
    "    If int, random_state is the seed used by the random number generator;\n",
    "    If RandomState instance, random_state is the random number generator;\n",
    "    If None, the random number generator is the RandomState instance used\n",
    "    by np.random.\n",
    "\n",
    "**shuffle** : boolean, optional (default=True)\n",
    "    Whether or not to shuffle the data before splitting. If shuffle=False\n",
    "    then stratify must be None.\n",
    "\n",
    "**stratify** : array-like or None (default=None)\n",
    "    If not None, data is split in a stratified fashion, using this as\n",
    "    the class labels.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aEKUby5MwOUc"
   },
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size=0.1, random_state=42, stratify=y)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9chq57lximz"
   },
   "source": [
    "Please note that the `stratify` parameter works only in the context of classification tasks where there are a fixed amount of possible outputs/targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mN9b1m-YEqVP"
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "In statistics, linear regression is a linear approach to modelling the relationship between a set a features, and a desired output. The case of one input feature is called simple linear regression; for more than one, the process is called multiple linear regression.\n",
    "\n",
    "Scikit Learn defines this algorithm in `LinearRegression` class as a part of the `linear_models` module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euPwTAlozl-z"
   },
   "source": [
    "First, we load the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "urCbsGeMEqVP",
    "outputId": "6e87b452-8a90-4ff1-e7db-690edf9a45bc"
   },
   "source": [
    "x,y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "print(\"Diabetes features/input shape:\", x.shape)\n",
    "print(\"Diabetes target/output shape:\", y.shape)"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diabetes features/input shape: (442, 10)\n",
      "Diabetes target/output shape: (442,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsXgJi2uzh96"
   },
   "source": [
    "Second, we split the data into 90/10 training/testing split (90% of the data will be used for training while 10% will be used for testing)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tgO8Mj4jlXdZ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bc2058a8-6a52-4bf1-96e6-a6bdb83dd140"
   },
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "((397, 10), (45, 10), (397,), (45,))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3L2rnqyzXzV"
   },
   "source": [
    "Third, we train (i.e. `fit`) the model using the training dataset (`x_train` as inputs, `y_train` as targets)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xy0-GOCqEqVQ",
    "outputId": "e1aa889f-8b3a-4395-f5a6-8322ff7282be"
   },
   "source": [
    "regressor = linear_model.LinearRegression()\n",
    "regressor.fit(x_train, y_train)\n",
    "\n",
    "# we can preview the learned coefficients (i.e. weights) and intercept (i.e. bias)\n",
    "\n",
    "print('Weights:\\n', regressor.coef_)\n",
    "print('Bias:\\n', regressor.intercept_)"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      " [  19.92576904 -262.55453086  509.19112446  336.09693678 -849.29530342\n",
      "  480.22076125  120.68418641  236.71853501  716.61035542   70.41045019]\n",
      "Bias:\n",
      " 151.7227046642232\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwY6r_g5zTMN"
   },
   "source": [
    "Fourth, we'll feed the test set into the trained model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SC2kNUdaAe7_"
   },
   "source": [
    "y_pred = regressor.predict(x_test)"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edEogEf-zRCl"
   },
   "source": [
    "Finally, we'll evaluate the predicted output against the ground-truth values in `y_test` using Scikit Learn's `metrics` module\n",
    "\n",
    "One of the most used metrics to evaluate regression models is `mean_squared_error` which has the following formula: $$\\frac{1}{n}\\sum_{i=1}^{n}(\\hat y_i - y_i)^2$$\n",
    "\n",
    "Where `n` is the total number of examples evaluated (in this case 45), $\\hat y$ is the predicted value (here `y_pred`) and $y$ is the ground-truth value (here `y_test`)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_2ma5hdDA6tX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c0292a24-e6bb-4903-a1c7-07778323a507"
   },
   "source": [
    "metrics.mean_squared_error(y_test, y_pred)"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "2743.8800467688443"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZSazzIi1xar"
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "In statistics, the logistic model (or logit model) is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc. Each object being detected in the image would be assigned a probability between 0 and 1, with a sum of one.\n",
    "\n",
    "\n",
    "Scikit Learn defines this algorithm in `LogisticRegression` class as a part of the `linear_models` module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lq8iQ7Sl1xas"
   },
   "source": [
    "First, we load the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YTS2cFdD1xas",
    "outputId": "0121839e-b697-4110-e67a-f7e52ce14b1f"
   },
   "source": [
    "x,y = datasets.load_breast_cancer(return_X_y=True)\n",
    "\n",
    "print(\"Breast Cancer features/input shape:\", x.shape)\n",
    "print(\"Breast Cancer target/output shape:\", y.shape)"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast Cancer features/input shape: (569, 30)\n",
      "Breast Cancer target/output shape: (569,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZIGpVe_1xat"
   },
   "source": [
    "Second, we split the data into 90/10 training/testing split (90% of the data will be used for training while 10% will be used for testing)\n",
    "\n",
    "Since this is a classification problem (we only have two possible outputs, 1 or 0), we can use the `stratify` parameter to ensure that the two possible output values are distributed proportionally between the training and testing sets and preserve the data's original distribution across the two sets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vh9i-abM1xat",
    "outputId": "cde947ff-9b8f-4f10-cba7-92ca2cd9243b"
   },
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "((512, 30), (57, 30), (512,), (57,))"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQBL3xRW1xau"
   },
   "source": [
    "Third, we train (i.e. `fit`) the model using the training dataset (`x_train` as inputs, `y_train` as targets)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_rv1UZf1xau",
    "outputId": "fb2d34b3-fddd-4521-8d28-92fd7f1689d4"
   },
   "source": [
    "classifier = linear_model.LogisticRegression()\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# we can preview the learned coefficients (i.e. weights) and intercept (i.e. bias)\n",
    "\n",
    "print('Weights:\\n', regressor.coef_)\n",
    "print('Bias:\\n', regressor.intercept_)"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      " [  19.92576904 -262.55453086  509.19112446  336.09693678 -849.29530342\n",
      "  480.22076125  120.68418641  236.71853501  716.61035542   70.41045019]\n",
      "Bias:\n",
      " 151.7227046642232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabri/Projects/Python/Code Lab/venv/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xE2N6-SR1xau"
   },
   "source": [
    "Fourth, we'll feed the test set into the trained model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hV_LmBvP1xav"
   },
   "source": [
    "y_pred = classifier.predict(x_test)"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReSoDvxc1xav"
   },
   "source": [
    "Finally, we'll evaluate the predicted output against the ground-truth values in `y_test` using Scikit Learn's `metrics` module\n",
    "\n",
    "One of the most used metrics to evaluate classification models is `accuracy_score` which calculates the precentage of the examples that the trained classifier guessed correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6WjQ-aSI1xav",
    "outputId": "ef937315-86c1-48c9-fb86-f83f0fbd830d"
   },
   "source": [
    "metrics.accuracy_score(y_test, y_pred)"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9298245614035088"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FCTSdrx24ml"
   },
   "source": [
    "Here the accuracy score corresponds to 94.7%"
   ]
  }
 ]
}