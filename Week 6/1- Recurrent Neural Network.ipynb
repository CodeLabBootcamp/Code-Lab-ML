{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Recurrent Neural Network",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3NY-r9Pvd6N"
      },
      "source": [
        "# Recurrent Neural Networks\n",
        "\n",
        "{{ badge }}\n",
        "\n",
        "A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.\n",
        "\n",
        "Keras API provides easy implementation for RNN layers, we'll go over them as we train a model for sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9YGJ4I95lfc"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import model_selection\n",
        "# We'll import the layers directly for easier model definition\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, Flatten, SimpleRNN, LSTM, GRU, Bidirectional"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syWAlrik73su"
      },
      "source": [
        "tf.random.set_seed(42)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkFpFPyz4F-h"
      },
      "source": [
        "# Below are some parameters that we'll set up now, don't worry about it for now\n",
        "\n",
        "vocab_size = 8000\n",
        "max_sequence = 128\n",
        "embeddings_dims = 100"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q5naQM5BSi3"
      },
      "source": [
        "## Download Data From Kaggle\n",
        "\n",
        "We'll be downloading the dataset from Kaggle, this requires using their download API, we'll go over the steps to do it.\n",
        "\n",
        "First we'll be uploading `kaggle.json` into the '~\\.kaggle` directory that we'll be creating, this will enable us to download datasets directly from Kaggle, more info on the process can be found here: https://github.com/Kaggle/kaggle-api "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sgut6-JNzUYK",
        "outputId": "cf644bcd-6ae5-46cf-a213-1ea8ba181fc9"
      },
      "source": [
        "# First we'll create a new folder to put kaggle.json into\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "# Let's confirm that the directory is created\n",
        "!cd /root/ && ls -la"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 64\n",
            "drwx------ 1 root root 4096 Feb  3 02:12 .\n",
            "drwxr-xr-x 1 root root 4096 Feb  3 01:46 ..\n",
            "-r-xr-xr-x 1 root root 1169 Jan  1  2000 .bashrc\n",
            "drwxr-xr-x 1 root root 4096 Feb  1 17:56 .cache\n",
            "drwxr-xr-x 1 root root 4096 Feb  1 17:54 .config\n",
            "drwxr-xr-x 3 root root 4096 Feb  1 17:28 .gsutil\n",
            "drwxr-xr-x 5 root root 4096 Feb  1 17:54 .ipython\n",
            "drwx------ 2 root root 4096 Feb  1 17:54 .jupyter\n",
            "drwxr-xr-x 2 root root 4096 Feb  3 02:12 .kaggle\n",
            "drwxr-xr-x 2 root root 4096 Feb  3 01:46 .keras\n",
            "drwx------ 1 root root 4096 Feb  1 17:54 .local\n",
            "drwxr-xr-x 4 root root 4096 Feb  1 17:54 .npm\n",
            "-rw-r--r-- 1 root root  148 Aug 17  2015 .profile\n",
            "-r-xr-xr-x 1 root root  254 Jan  1  2000 .tmux.conf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpp0SxIHzsr9"
      },
      "source": [
        "Now that `.kaggle` directory is all set up, we'll need to upload the `kaggle.json` file. For this you'll need a Kaggle account, you can obtain the file from this url `https://www.kaggle.com/<username>/account` (make sure to replace <username> with your actual username).\n",
        "\n",
        "More info here: https://github.com/Kaggle/kaggle-api"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "4MN7ajAhzdm3",
        "outputId": "f32cf229-1b3b-4b1d-8b14-f2d56957b6f7"
      },
      "source": [
        "# Import colab's files module\n",
        "from google.colab import files\n",
        "\n",
        "# Start the upload, this will open the upload prompt below\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Confirm that we've uploaded the kaggle.json file\n",
        "print(\"Uploaded File:\", list(uploaded.keys())[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-65b50058-542c-42da-a92e-42136c550abe\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-65b50058-542c-42da-a92e-42136c550abe\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Uploaded File: kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HciwJA6sz9_8"
      },
      "source": [
        "Now that we have the `kaggle.json` file uploaded, we'll need to move it to `.kaggle` directory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75ycL-Es1Fr1",
        "outputId": "fc2e9db9-475c-44c5-c556-faa6428399a7"
      },
      "source": [
        "# Move kaggle.json to .kaggle directory\n",
        "!mv kaggle.json /root/.kaggle/kaggle.json\n",
        "\n",
        "# Change file permission to allow python to access it\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# List files inside .kaggle to confirm that the file is moved\n",
        "!cd /root/.kaggle && ls -la"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 16\n",
            "drwxr-xr-x 2 root root 4096 Feb  3 02:12 .\n",
            "drwx------ 1 root root 4096 Feb  3 02:12 ..\n",
            "-rw------- 1 root root   64 Feb  3 02:12 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20I8vBn32C2c"
      },
      "source": [
        "And finally, we can download the dataset directly from kaggle using their Python API command (note that you may need to run `!pip install kaggle`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPr6yI-ny4MK",
        "outputId": "849544e9-c511-46ef-82bd-b823bd523d03"
      },
      "source": [
        "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading imdb-dataset-of-50k-movie-reviews.zip to /content\n",
            " 35% 9.00M/25.7M [00:00<00:00, 24.2MB/s]\n",
            "100% 25.7M/25.7M [00:00<00:00, 52.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EZbPdXc2gFn"
      },
      "source": [
        "And now, let's unzip the downloaded file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37nxLwqXqMV0",
        "outputId": "6ab1484b-3826-44f8-be19-63278c49047e"
      },
      "source": [
        "!unzip imdb-dataset-of-50k-movie-reviews.zip"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  imdb-dataset-of-50k-movie-reviews.zip\n",
            "  inflating: IMDB Dataset.csv        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMaRHY1K2kOC"
      },
      "source": [
        "## Load & Preprocess Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP92gYo02wdb"
      },
      "source": [
        "First, we'll load the extracted csv file into a dataframe to examine the content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "LfwWcAtf6y4Y",
        "outputId": "ebfc9105-0dd8-44bf-b09f-01ce0051f85b"
      },
      "source": [
        "df = pd.read_csv('IMDB Dataset.csv')\n",
        "df"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>I thought this movie did a down right good job...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>I am a Catholic taught in parochial elementary...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>I'm going to have to disagree with the previou...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>No one expects the Star Trek movies to be high...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review sentiment\n",
              "0      One of the other reviewers has mentioned that ...  positive\n",
              "1      A wonderful little production. <br /><br />The...  positive\n",
              "2      I thought this was a wonderful way to spend ti...  positive\n",
              "3      Basically there's a family where a little boy ...  negative\n",
              "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
              "...                                                  ...       ...\n",
              "49995  I thought this movie did a down right good job...  positive\n",
              "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
              "49997  I am a Catholic taught in parochial elementary...  negative\n",
              "49998  I'm going to have to disagree with the previou...  negative\n",
              "49999  No one expects the Star Trek movies to be high...  negative\n",
              "\n",
              "[50000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqe9RiDM2tF1"
      },
      "source": [
        "We can see that we have two columns, the review which has the review text that will be used as the input for the model, and sentiment, which is what the model will try to predict, let's split them into X's and y's  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHO-W2dX_H6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0abd68-22fc-4da1-b97c-ef36dedf1436"
      },
      "source": [
        "x = df.review\n",
        "y = df.sentiment\n",
        "\n",
        "x.shape, y.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000,), (50000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItvMTpgJ3OYS"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "Since we know that neural network don't work with text, we'll need a way to prepare the review text to be consumed by the network.\n",
        "\n",
        "Keras provides APIs for preparing text that can be fit and reused to prepare multiple text documents. This may be the preferred approach for large projects.\n",
        "\n",
        "We'll first use the `Tokenizer` class from `tf.keras.preprocessing.text` module to convert tokens (i.e. words, symbols, numbers...etc.) into numbers that can be consumed by neural networks.\n",
        "\n",
        "You can read the documentation of the `Tokenizer` class here: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgQ7aDve_Q4R"
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    num_words=vocab_size, # Maximum number of tokens to include, we'll use vocab_size that we defined earlier\n",
        "    oov_token='<OOV>', # A token that will replace words that will not be in the limited vocabulary set by vocab_size  \n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n' # Symbols that will be removed from the texts\n",
        ")\n",
        "\n",
        "# Now we will train the tokenizer on our datasets, this allows the tokenizer to learn the most frequent words and create an index for them\n",
        "\n",
        "tokenizer.fit_on_texts(x)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-CVDeAu5Bt2"
      },
      "source": [
        "Now we can use the tokenizer to convert texts into sequences of numbers (each token/word has its own unique index)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVtTiS_iW-Ne",
        "outputId": "e96e2864-a814-49b4-eb83-75a2b7e696eb"
      },
      "source": [
        "tokenizer.word_index"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<OOV>': 1,\n",
              " 'the': 2,\n",
              " 'and': 3,\n",
              " 'a': 4,\n",
              " 'of': 5,\n",
              " 'to': 6,\n",
              " 'is': 7,\n",
              " 'br': 8,\n",
              " 'in': 9,\n",
              " 'it': 10,\n",
              " 'i': 11,\n",
              " 'this': 12,\n",
              " 'that': 13,\n",
              " 'was': 14,\n",
              " 'as': 15,\n",
              " 'for': 16,\n",
              " 'with': 17,\n",
              " 'movie': 18,\n",
              " 'but': 19,\n",
              " 'film': 20,\n",
              " 'on': 21,\n",
              " 'not': 22,\n",
              " 'you': 23,\n",
              " 'are': 24,\n",
              " 'his': 25,\n",
              " 'have': 26,\n",
              " 'be': 27,\n",
              " 'one': 28,\n",
              " 'he': 29,\n",
              " 'all': 30,\n",
              " 'at': 31,\n",
              " 'by': 32,\n",
              " 'an': 33,\n",
              " 'they': 34,\n",
              " 'so': 35,\n",
              " 'who': 36,\n",
              " 'from': 37,\n",
              " 'like': 38,\n",
              " 'or': 39,\n",
              " 'just': 40,\n",
              " 'her': 41,\n",
              " 'out': 42,\n",
              " 'about': 43,\n",
              " 'if': 44,\n",
              " \"it's\": 45,\n",
              " 'has': 46,\n",
              " 'there': 47,\n",
              " 'some': 48,\n",
              " 'what': 49,\n",
              " 'good': 50,\n",
              " 'when': 51,\n",
              " 'more': 52,\n",
              " 'very': 53,\n",
              " 'up': 54,\n",
              " 'no': 55,\n",
              " 'time': 56,\n",
              " 'my': 57,\n",
              " 'even': 58,\n",
              " 'would': 59,\n",
              " 'she': 60,\n",
              " 'which': 61,\n",
              " 'only': 62,\n",
              " 'really': 63,\n",
              " 'see': 64,\n",
              " 'story': 65,\n",
              " 'their': 66,\n",
              " 'had': 67,\n",
              " 'can': 68,\n",
              " 'me': 69,\n",
              " 'well': 70,\n",
              " 'were': 71,\n",
              " 'than': 72,\n",
              " 'much': 73,\n",
              " 'we': 74,\n",
              " 'bad': 75,\n",
              " 'been': 76,\n",
              " 'get': 77,\n",
              " 'do': 78,\n",
              " 'great': 79,\n",
              " 'other': 80,\n",
              " 'will': 81,\n",
              " 'also': 82,\n",
              " 'into': 83,\n",
              " 'people': 84,\n",
              " 'because': 85,\n",
              " 'how': 86,\n",
              " 'first': 87,\n",
              " 'him': 88,\n",
              " 'most': 89,\n",
              " \"don't\": 90,\n",
              " 'made': 91,\n",
              " 'then': 92,\n",
              " 'its': 93,\n",
              " 'them': 94,\n",
              " 'make': 95,\n",
              " 'way': 96,\n",
              " 'too': 97,\n",
              " 'movies': 98,\n",
              " 'could': 99,\n",
              " 'any': 100,\n",
              " 'after': 101,\n",
              " 'think': 102,\n",
              " 'characters': 103,\n",
              " 'watch': 104,\n",
              " 'films': 105,\n",
              " 'two': 106,\n",
              " 'many': 107,\n",
              " 'seen': 108,\n",
              " 'character': 109,\n",
              " 'being': 110,\n",
              " 'never': 111,\n",
              " 'plot': 112,\n",
              " 'love': 113,\n",
              " 'acting': 114,\n",
              " 'life': 115,\n",
              " 'did': 116,\n",
              " 'best': 117,\n",
              " 'where': 118,\n",
              " 'know': 119,\n",
              " 'show': 120,\n",
              " 'little': 121,\n",
              " 'over': 122,\n",
              " 'off': 123,\n",
              " 'ever': 124,\n",
              " 'does': 125,\n",
              " 'your': 126,\n",
              " 'better': 127,\n",
              " 'end': 128,\n",
              " 'man': 129,\n",
              " 'scene': 130,\n",
              " 'still': 131,\n",
              " 'say': 132,\n",
              " 'these': 133,\n",
              " 'here': 134,\n",
              " 'scenes': 135,\n",
              " 'why': 136,\n",
              " 'while': 137,\n",
              " 'something': 138,\n",
              " 'such': 139,\n",
              " 'go': 140,\n",
              " 'through': 141,\n",
              " 'back': 142,\n",
              " 'should': 143,\n",
              " 'those': 144,\n",
              " 'real': 145,\n",
              " \"i'm\": 146,\n",
              " 'now': 147,\n",
              " 'watching': 148,\n",
              " 'thing': 149,\n",
              " \"doesn't\": 150,\n",
              " 'actors': 151,\n",
              " 'though': 152,\n",
              " 'funny': 153,\n",
              " 'years': 154,\n",
              " \"didn't\": 155,\n",
              " 'old': 156,\n",
              " '10': 157,\n",
              " 'another': 158,\n",
              " 'work': 159,\n",
              " 'before': 160,\n",
              " 'actually': 161,\n",
              " 'nothing': 162,\n",
              " 'makes': 163,\n",
              " 'look': 164,\n",
              " 'director': 165,\n",
              " 'find': 166,\n",
              " 'going': 167,\n",
              " 'same': 168,\n",
              " 'new': 169,\n",
              " 'lot': 170,\n",
              " 'every': 171,\n",
              " 'few': 172,\n",
              " 'again': 173,\n",
              " 'part': 174,\n",
              " 'cast': 175,\n",
              " 'down': 176,\n",
              " 'us': 177,\n",
              " 'things': 178,\n",
              " 'want': 179,\n",
              " 'quite': 180,\n",
              " 'pretty': 181,\n",
              " 'world': 182,\n",
              " 'horror': 183,\n",
              " 'around': 184,\n",
              " 'seems': 185,\n",
              " \"can't\": 186,\n",
              " 'young': 187,\n",
              " 'take': 188,\n",
              " 'however': 189,\n",
              " 'got': 190,\n",
              " 'thought': 191,\n",
              " 'big': 192,\n",
              " 'fact': 193,\n",
              " 'enough': 194,\n",
              " 'long': 195,\n",
              " 'both': 196,\n",
              " \"that's\": 197,\n",
              " 'give': 198,\n",
              " \"i've\": 199,\n",
              " 'own': 200,\n",
              " 'may': 201,\n",
              " 'between': 202,\n",
              " 'comedy': 203,\n",
              " 'right': 204,\n",
              " 'series': 205,\n",
              " 'action': 206,\n",
              " 'must': 207,\n",
              " 'music': 208,\n",
              " 'without': 209,\n",
              " 'times': 210,\n",
              " 'saw': 211,\n",
              " 'always': 212,\n",
              " 'original': 213,\n",
              " \"isn't\": 214,\n",
              " 'role': 215,\n",
              " 'come': 216,\n",
              " 'almost': 217,\n",
              " 'gets': 218,\n",
              " 'interesting': 219,\n",
              " 'guy': 220,\n",
              " 'point': 221,\n",
              " 'done': 222,\n",
              " \"there's\": 223,\n",
              " 'whole': 224,\n",
              " 'least': 225,\n",
              " 'far': 226,\n",
              " 'bit': 227,\n",
              " 'script': 228,\n",
              " 'minutes': 229,\n",
              " 'feel': 230,\n",
              " '2': 231,\n",
              " 'anything': 232,\n",
              " 'making': 233,\n",
              " 'might': 234,\n",
              " 'since': 235,\n",
              " 'am': 236,\n",
              " 'family': 237,\n",
              " \"he's\": 238,\n",
              " 'last': 239,\n",
              " 'probably': 240,\n",
              " 'tv': 241,\n",
              " 'performance': 242,\n",
              " 'kind': 243,\n",
              " 'away': 244,\n",
              " 'yet': 245,\n",
              " 'fun': 246,\n",
              " 'worst': 247,\n",
              " 'sure': 248,\n",
              " 'rather': 249,\n",
              " 'hard': 250,\n",
              " 'anyone': 251,\n",
              " 'girl': 252,\n",
              " 'each': 253,\n",
              " 'played': 254,\n",
              " 'day': 255,\n",
              " 'found': 256,\n",
              " 'looking': 257,\n",
              " 'woman': 258,\n",
              " 'screen': 259,\n",
              " 'although': 260,\n",
              " 'our': 261,\n",
              " 'especially': 262,\n",
              " 'believe': 263,\n",
              " 'having': 264,\n",
              " 'trying': 265,\n",
              " 'course': 266,\n",
              " 'dvd': 267,\n",
              " 'everything': 268,\n",
              " 'set': 269,\n",
              " 'goes': 270,\n",
              " 'comes': 271,\n",
              " 'put': 272,\n",
              " 'ending': 273,\n",
              " 'maybe': 274,\n",
              " 'place': 275,\n",
              " 'book': 276,\n",
              " 'shows': 277,\n",
              " 'three': 278,\n",
              " 'worth': 279,\n",
              " 'different': 280,\n",
              " 'main': 281,\n",
              " 'once': 282,\n",
              " 'sense': 283,\n",
              " 'american': 284,\n",
              " 'reason': 285,\n",
              " 'looks': 286,\n",
              " 'effects': 287,\n",
              " 'watched': 288,\n",
              " 'play': 289,\n",
              " 'true': 290,\n",
              " 'money': 291,\n",
              " 'actor': 292,\n",
              " \"wasn't\": 293,\n",
              " 'job': 294,\n",
              " 'together': 295,\n",
              " 'war': 296,\n",
              " 'someone': 297,\n",
              " 'plays': 298,\n",
              " 'instead': 299,\n",
              " 'high': 300,\n",
              " 'during': 301,\n",
              " 'year': 302,\n",
              " 'said': 303,\n",
              " 'half': 304,\n",
              " 'everyone': 305,\n",
              " 'later': 306,\n",
              " 'takes': 307,\n",
              " '1': 308,\n",
              " 'seem': 309,\n",
              " 'audience': 310,\n",
              " 'special': 311,\n",
              " 'beautiful': 312,\n",
              " 'left': 313,\n",
              " 'himself': 314,\n",
              " 'seeing': 315,\n",
              " 'john': 316,\n",
              " 'night': 317,\n",
              " 'black': 318,\n",
              " 'version': 319,\n",
              " 'shot': 320,\n",
              " 'excellent': 321,\n",
              " 'idea': 322,\n",
              " 'house': 323,\n",
              " 'mind': 324,\n",
              " 'star': 325,\n",
              " 'wife': 326,\n",
              " 'fan': 327,\n",
              " 'death': 328,\n",
              " 'used': 329,\n",
              " 'else': 330,\n",
              " 'simply': 331,\n",
              " 'nice': 332,\n",
              " 'budget': 333,\n",
              " 'poor': 334,\n",
              " 'short': 335,\n",
              " 'completely': 336,\n",
              " 'second': 337,\n",
              " \"you're\": 338,\n",
              " '3': 339,\n",
              " 'read': 340,\n",
              " 'less': 341,\n",
              " 'along': 342,\n",
              " 'top': 343,\n",
              " 'help': 344,\n",
              " 'home': 345,\n",
              " 'men': 346,\n",
              " 'either': 347,\n",
              " 'line': 348,\n",
              " 'boring': 349,\n",
              " 'dead': 350,\n",
              " 'friends': 351,\n",
              " 'kids': 352,\n",
              " 'try': 353,\n",
              " 'production': 354,\n",
              " 'enjoy': 355,\n",
              " 'camera': 356,\n",
              " 'use': 357,\n",
              " 'wrong': 358,\n",
              " 'given': 359,\n",
              " 'low': 360,\n",
              " 'classic': 361,\n",
              " 'father': 362,\n",
              " 'need': 363,\n",
              " 'full': 364,\n",
              " 'stupid': 365,\n",
              " 'next': 366,\n",
              " 'until': 367,\n",
              " 'performances': 368,\n",
              " 'school': 369,\n",
              " 'hollywood': 370,\n",
              " 'rest': 371,\n",
              " 'truly': 372,\n",
              " 'awful': 373,\n",
              " 'video': 374,\n",
              " 'couple': 375,\n",
              " 'start': 376,\n",
              " 'sex': 377,\n",
              " 'recommend': 378,\n",
              " 'women': 379,\n",
              " 'let': 380,\n",
              " 'tell': 381,\n",
              " 'terrible': 382,\n",
              " 'remember': 383,\n",
              " 'mean': 384,\n",
              " 'came': 385,\n",
              " 'getting': 386,\n",
              " 'understand': 387,\n",
              " 'perhaps': 388,\n",
              " 'moments': 389,\n",
              " 'name': 390,\n",
              " 'keep': 391,\n",
              " 'face': 392,\n",
              " 'itself': 393,\n",
              " 'wonderful': 394,\n",
              " 'playing': 395,\n",
              " 'human': 396,\n",
              " 'style': 397,\n",
              " 'small': 398,\n",
              " 'episode': 399,\n",
              " 'perfect': 400,\n",
              " 'others': 401,\n",
              " 'person': 402,\n",
              " 'doing': 403,\n",
              " 'often': 404,\n",
              " 'early': 405,\n",
              " 'stars': 406,\n",
              " 'definitely': 407,\n",
              " 'written': 408,\n",
              " 'head': 409,\n",
              " 'lines': 410,\n",
              " 'dialogue': 411,\n",
              " 'gives': 412,\n",
              " 'piece': 413,\n",
              " \"couldn't\": 414,\n",
              " 'went': 415,\n",
              " 'finally': 416,\n",
              " 'mother': 417,\n",
              " 'case': 418,\n",
              " 'title': 419,\n",
              " 'absolutely': 420,\n",
              " 'boy': 421,\n",
              " 'live': 422,\n",
              " 'yes': 423,\n",
              " 'laugh': 424,\n",
              " 'certainly': 425,\n",
              " 'liked': 426,\n",
              " 'become': 427,\n",
              " 'entertaining': 428,\n",
              " 'worse': 429,\n",
              " 'oh': 430,\n",
              " 'sort': 431,\n",
              " 'loved': 432,\n",
              " 'lost': 433,\n",
              " 'called': 434,\n",
              " 'hope': 435,\n",
              " 'picture': 436,\n",
              " 'felt': 437,\n",
              " 'overall': 438,\n",
              " 'entire': 439,\n",
              " 'mr': 440,\n",
              " 'several': 441,\n",
              " 'based': 442,\n",
              " 'supposed': 443,\n",
              " 'cinema': 444,\n",
              " 'friend': 445,\n",
              " 'guys': 446,\n",
              " 'sound': 447,\n",
              " '5': 448,\n",
              " 'problem': 449,\n",
              " 'drama': 450,\n",
              " 'against': 451,\n",
              " 'waste': 452,\n",
              " 'white': 453,\n",
              " 'beginning': 454,\n",
              " '4': 455,\n",
              " 'fans': 456,\n",
              " 'totally': 457,\n",
              " 'dark': 458,\n",
              " 'care': 459,\n",
              " 'direction': 460,\n",
              " 'humor': 461,\n",
              " 'wanted': 462,\n",
              " \"she's\": 463,\n",
              " 'seemed': 464,\n",
              " 'under': 465,\n",
              " 'game': 466,\n",
              " 'children': 467,\n",
              " 'despite': 468,\n",
              " 'lives': 469,\n",
              " 'lead': 470,\n",
              " 'guess': 471,\n",
              " 'example': 472,\n",
              " 'already': 473,\n",
              " 'final': 474,\n",
              " \"you'll\": 475,\n",
              " 'throughout': 476,\n",
              " 'evil': 477,\n",
              " 'turn': 478,\n",
              " 'becomes': 479,\n",
              " 'unfortunately': 480,\n",
              " 'able': 481,\n",
              " 'quality': 482,\n",
              " \"i'd\": 483,\n",
              " 'days': 484,\n",
              " 'history': 485,\n",
              " 'fine': 486,\n",
              " 'side': 487,\n",
              " 'wants': 488,\n",
              " 'horrible': 489,\n",
              " 'heart': 490,\n",
              " 'writing': 491,\n",
              " 'amazing': 492,\n",
              " 'b': 493,\n",
              " 'flick': 494,\n",
              " 'killer': 495,\n",
              " 'run': 496,\n",
              " 'son': 497,\n",
              " '\\x96': 498,\n",
              " 'michael': 499,\n",
              " 'works': 500,\n",
              " 'close': 501,\n",
              " \"they're\": 502,\n",
              " 'act': 503,\n",
              " 'art': 504,\n",
              " 'kill': 505,\n",
              " 'matter': 506,\n",
              " 'etc': 507,\n",
              " 'tries': 508,\n",
              " \"won't\": 509,\n",
              " 'past': 510,\n",
              " 'town': 511,\n",
              " 'enjoyed': 512,\n",
              " 'turns': 513,\n",
              " 'brilliant': 514,\n",
              " 'gave': 515,\n",
              " 'behind': 516,\n",
              " 'parts': 517,\n",
              " 'stuff': 518,\n",
              " 'genre': 519,\n",
              " 'eyes': 520,\n",
              " 'car': 521,\n",
              " 'favorite': 522,\n",
              " 'directed': 523,\n",
              " 'late': 524,\n",
              " 'hand': 525,\n",
              " 'expect': 526,\n",
              " 'soon': 527,\n",
              " 'hour': 528,\n",
              " 'obviously': 529,\n",
              " 'themselves': 530,\n",
              " 'sometimes': 531,\n",
              " 'killed': 532,\n",
              " 'thinking': 533,\n",
              " 'actress': 534,\n",
              " 'child': 535,\n",
              " 'girls': 536,\n",
              " 'viewer': 537,\n",
              " 'starts': 538,\n",
              " 'city': 539,\n",
              " 'myself': 540,\n",
              " 'decent': 541,\n",
              " 'highly': 542,\n",
              " 'stop': 543,\n",
              " 'type': 544,\n",
              " 'self': 545,\n",
              " 'god': 546,\n",
              " 'says': 547,\n",
              " 'group': 548,\n",
              " 'anyway': 549,\n",
              " 'voice': 550,\n",
              " 'took': 551,\n",
              " 'known': 552,\n",
              " 'blood': 553,\n",
              " 'kid': 554,\n",
              " 'heard': 555,\n",
              " 'happens': 556,\n",
              " 'except': 557,\n",
              " 'fight': 558,\n",
              " 'feeling': 559,\n",
              " 'experience': 560,\n",
              " 'coming': 561,\n",
              " 'slow': 562,\n",
              " 'daughter': 563,\n",
              " 'writer': 564,\n",
              " 'stories': 565,\n",
              " 'moment': 566,\n",
              " 'leave': 567,\n",
              " 'told': 568,\n",
              " 'extremely': 569,\n",
              " 'score': 570,\n",
              " 'violence': 571,\n",
              " 'involved': 572,\n",
              " 'police': 573,\n",
              " 'strong': 574,\n",
              " 'lack': 575,\n",
              " 'chance': 576,\n",
              " 'cannot': 577,\n",
              " 'hit': 578,\n",
              " 'roles': 579,\n",
              " 'hilarious': 580,\n",
              " 's': 581,\n",
              " 'wonder': 582,\n",
              " 'happen': 583,\n",
              " 'particularly': 584,\n",
              " 'ok': 585,\n",
              " 'including': 586,\n",
              " 'save': 587,\n",
              " 'living': 588,\n",
              " 'looked': 589,\n",
              " \"wouldn't\": 590,\n",
              " 'crap': 591,\n",
              " 'please': 592,\n",
              " 'simple': 593,\n",
              " 'murder': 594,\n",
              " 'cool': 595,\n",
              " 'obvious': 596,\n",
              " 'happened': 597,\n",
              " 'complete': 598,\n",
              " 'cut': 599,\n",
              " 'serious': 600,\n",
              " 'age': 601,\n",
              " 'gore': 602,\n",
              " 'attempt': 603,\n",
              " 'hell': 604,\n",
              " 'ago': 605,\n",
              " 'song': 606,\n",
              " 'shown': 607,\n",
              " 'taken': 608,\n",
              " 'english': 609,\n",
              " 'james': 610,\n",
              " 'robert': 611,\n",
              " 'david': 612,\n",
              " 'seriously': 613,\n",
              " 'released': 614,\n",
              " 'reality': 615,\n",
              " 'opening': 616,\n",
              " 'jokes': 617,\n",
              " 'interest': 618,\n",
              " 'across': 619,\n",
              " 'none': 620,\n",
              " 'hero': 621,\n",
              " 'exactly': 622,\n",
              " 'today': 623,\n",
              " 'possible': 624,\n",
              " 'alone': 625,\n",
              " 'sad': 626,\n",
              " 'brother': 627,\n",
              " 'number': 628,\n",
              " 'career': 629,\n",
              " 'saying': 630,\n",
              " \"film's\": 631,\n",
              " 'usually': 632,\n",
              " 'hours': 633,\n",
              " 'cinematography': 634,\n",
              " 'talent': 635,\n",
              " 'view': 636,\n",
              " 'annoying': 637,\n",
              " 'running': 638,\n",
              " 'yourself': 639,\n",
              " 'relationship': 640,\n",
              " 'documentary': 641,\n",
              " 'wish': 642,\n",
              " 'order': 643,\n",
              " 'huge': 644,\n",
              " 'shots': 645,\n",
              " 'whose': 646,\n",
              " 'ridiculous': 647,\n",
              " 'taking': 648,\n",
              " 'important': 649,\n",
              " 'light': 650,\n",
              " 'body': 651,\n",
              " 'middle': 652,\n",
              " 'level': 653,\n",
              " 'ends': 654,\n",
              " 'started': 655,\n",
              " 'female': 656,\n",
              " 'call': 657,\n",
              " \"i'll\": 658,\n",
              " 'husband': 659,\n",
              " 'four': 660,\n",
              " 'power': 661,\n",
              " 'word': 662,\n",
              " 'turned': 663,\n",
              " 'major': 664,\n",
              " 'opinion': 665,\n",
              " 'change': 666,\n",
              " 'mostly': 667,\n",
              " 'usual': 668,\n",
              " 'scary': 669,\n",
              " 'silly': 670,\n",
              " 'rating': 671,\n",
              " 'beyond': 672,\n",
              " 'somewhat': 673,\n",
              " 'happy': 674,\n",
              " 'ones': 675,\n",
              " 'words': 676,\n",
              " 'room': 677,\n",
              " 'knows': 678,\n",
              " 'knew': 679,\n",
              " 'country': 680,\n",
              " 'disappointed': 681,\n",
              " 'talking': 682,\n",
              " 'novel': 683,\n",
              " 'apparently': 684,\n",
              " 'non': 685,\n",
              " 'strange': 686,\n",
              " 'upon': 687,\n",
              " 'attention': 688,\n",
              " 'basically': 689,\n",
              " 'single': 690,\n",
              " 'finds': 691,\n",
              " 'cheap': 692,\n",
              " 'modern': 693,\n",
              " 'due': 694,\n",
              " 'jack': 695,\n",
              " 'television': 696,\n",
              " 'musical': 697,\n",
              " 'problems': 698,\n",
              " 'miss': 699,\n",
              " 'episodes': 700,\n",
              " 'clearly': 701,\n",
              " 'local': 702,\n",
              " '7': 703,\n",
              " 'british': 704,\n",
              " 'thriller': 705,\n",
              " 'talk': 706,\n",
              " 'events': 707,\n",
              " 'sequence': 708,\n",
              " 'five': 709,\n",
              " \"aren't\": 710,\n",
              " 'class': 711,\n",
              " 'french': 712,\n",
              " 'moving': 713,\n",
              " 'ten': 714,\n",
              " 'fast': 715,\n",
              " 'earth': 716,\n",
              " 'review': 717,\n",
              " 'tells': 718,\n",
              " 'predictable': 719,\n",
              " 'songs': 720,\n",
              " 'team': 721,\n",
              " 'comic': 722,\n",
              " 'straight': 723,\n",
              " '8': 724,\n",
              " 'whether': 725,\n",
              " 'die': 726,\n",
              " 'add': 727,\n",
              " 'dialog': 728,\n",
              " 'entertainment': 729,\n",
              " 'above': 730,\n",
              " 'sets': 731,\n",
              " 'future': 732,\n",
              " 'enjoyable': 733,\n",
              " 'appears': 734,\n",
              " 'near': 735,\n",
              " 'space': 736,\n",
              " 'easily': 737,\n",
              " 'hate': 738,\n",
              " 'soundtrack': 739,\n",
              " 'bring': 740,\n",
              " 'giving': 741,\n",
              " 'lots': 742,\n",
              " 'similar': 743,\n",
              " 'romantic': 744,\n",
              " 'george': 745,\n",
              " 'supporting': 746,\n",
              " 'release': 747,\n",
              " 'mention': 748,\n",
              " 'within': 749,\n",
              " 'filmed': 750,\n",
              " 'message': 751,\n",
              " 'sequel': 752,\n",
              " 'clear': 753,\n",
              " 'falls': 754,\n",
              " \"haven't\": 755,\n",
              " 'needs': 756,\n",
              " 'dull': 757,\n",
              " 'suspense': 758,\n",
              " 'bunch': 759,\n",
              " 'eye': 760,\n",
              " 'surprised': 761,\n",
              " 'showing': 762,\n",
              " 'tried': 763,\n",
              " 'sorry': 764,\n",
              " 'certain': 765,\n",
              " 'working': 766,\n",
              " 'easy': 767,\n",
              " 'ways': 768,\n",
              " 'theme': 769,\n",
              " 'theater': 770,\n",
              " 'among': 771,\n",
              " 'named': 772,\n",
              " \"what's\": 773,\n",
              " 'storyline': 774,\n",
              " 'monster': 775,\n",
              " 'king': 776,\n",
              " 'stay': 777,\n",
              " 'effort': 778,\n",
              " 'minute': 779,\n",
              " 'fall': 780,\n",
              " 'stand': 781,\n",
              " 'gone': 782,\n",
              " 'rock': 783,\n",
              " 'using': 784,\n",
              " '9': 785,\n",
              " 'feature': 786,\n",
              " 'comments': 787,\n",
              " 'buy': 788,\n",
              " \"'\": 789,\n",
              " 'typical': 790,\n",
              " 't': 791,\n",
              " 'editing': 792,\n",
              " 'sister': 793,\n",
              " 'tale': 794,\n",
              " 'avoid': 795,\n",
              " 'dr': 796,\n",
              " 'mystery': 797,\n",
              " 'deal': 798,\n",
              " 'doubt': 799,\n",
              " 'fantastic': 800,\n",
              " 'kept': 801,\n",
              " 'nearly': 802,\n",
              " 'feels': 803,\n",
              " 'subject': 804,\n",
              " 'okay': 805,\n",
              " 'viewing': 806,\n",
              " 'elements': 807,\n",
              " 'oscar': 808,\n",
              " 'check': 809,\n",
              " 'points': 810,\n",
              " 'realistic': 811,\n",
              " 'means': 812,\n",
              " 'greatest': 813,\n",
              " 'herself': 814,\n",
              " 'parents': 815,\n",
              " 'famous': 816,\n",
              " 'imagine': 817,\n",
              " 'rent': 818,\n",
              " 'viewers': 819,\n",
              " 'richard': 820,\n",
              " 'crime': 821,\n",
              " 'form': 822,\n",
              " 'peter': 823,\n",
              " 'actual': 824,\n",
              " 'lady': 825,\n",
              " 'general': 826,\n",
              " 'dog': 827,\n",
              " 'follow': 828,\n",
              " 'believable': 829,\n",
              " 'period': 830,\n",
              " 'red': 831,\n",
              " 'brought': 832,\n",
              " 'move': 833,\n",
              " 'material': 834,\n",
              " 'forget': 835,\n",
              " 'somehow': 836,\n",
              " 'begins': 837,\n",
              " 're': 838,\n",
              " 'reviews': 839,\n",
              " 'animation': 840,\n",
              " 'paul': 841,\n",
              " \"you've\": 842,\n",
              " 'leads': 843,\n",
              " 'weak': 844,\n",
              " 'figure': 845,\n",
              " 'surprise': 846,\n",
              " 'hear': 847,\n",
              " 'sit': 848,\n",
              " 'average': 849,\n",
              " 'open': 850,\n",
              " 'sequences': 851,\n",
              " 'killing': 852,\n",
              " 'atmosphere': 853,\n",
              " 'eventually': 854,\n",
              " 'tom': 855,\n",
              " 'learn': 856,\n",
              " 'premise': 857,\n",
              " 'wait': 858,\n",
              " '20': 859,\n",
              " 'sci': 860,\n",
              " 'deep': 861,\n",
              " 'fi': 862,\n",
              " 'expected': 863,\n",
              " 'whatever': 864,\n",
              " 'indeed': 865,\n",
              " 'lame': 866,\n",
              " 'poorly': 867,\n",
              " 'particular': 868,\n",
              " 'note': 869,\n",
              " 'dance': 870,\n",
              " 'imdb': 871,\n",
              " 'shame': 872,\n",
              " 'situation': 873,\n",
              " 'third': 874,\n",
              " 'york': 875,\n",
              " 'box': 876,\n",
              " 'truth': 877,\n",
              " 'decided': 878,\n",
              " 'free': 879,\n",
              " 'hot': 880,\n",
              " \"who's\": 881,\n",
              " 'difficult': 882,\n",
              " 'needed': 883,\n",
              " 'season': 884,\n",
              " 'acted': 885,\n",
              " 'leaves': 886,\n",
              " 'unless': 887,\n",
              " 'romance': 888,\n",
              " 'emotional': 889,\n",
              " 'possibly': 890,\n",
              " 'gay': 891,\n",
              " 'sexual': 892,\n",
              " 'boys': 893,\n",
              " 'footage': 894,\n",
              " 'write': 895,\n",
              " 'western': 896,\n",
              " 'credits': 897,\n",
              " 'forced': 898,\n",
              " 'memorable': 899,\n",
              " 'doctor': 900,\n",
              " 'reading': 901,\n",
              " 'became': 902,\n",
              " 'otherwise': 903,\n",
              " 'air': 904,\n",
              " 'begin': 905,\n",
              " 'de': 906,\n",
              " 'crew': 907,\n",
              " 'question': 908,\n",
              " 'meet': 909,\n",
              " 'society': 910,\n",
              " 'male': 911,\n",
              " \"let's\": 912,\n",
              " 'meets': 913,\n",
              " 'plus': 914,\n",
              " 'cheesy': 915,\n",
              " 'hands': 916,\n",
              " 'superb': 917,\n",
              " 'screenplay': 918,\n",
              " 'beauty': 919,\n",
              " 'interested': 920,\n",
              " 'street': 921,\n",
              " 'features': 922,\n",
              " 'perfectly': 923,\n",
              " 'masterpiece': 924,\n",
              " 'whom': 925,\n",
              " 'laughs': 926,\n",
              " 'nature': 927,\n",
              " 'stage': 928,\n",
              " 'effect': 929,\n",
              " 'forward': 930,\n",
              " 'comment': 931,\n",
              " 'nor': 932,\n",
              " 'previous': 933,\n",
              " 'e': 934,\n",
              " 'badly': 935,\n",
              " 'sounds': 936,\n",
              " 'japanese': 937,\n",
              " 'weird': 938,\n",
              " 'island': 939,\n",
              " 'inside': 940,\n",
              " 'personal': 941,\n",
              " 'quickly': 942,\n",
              " 'total': 943,\n",
              " 'keeps': 944,\n",
              " 'towards': 945,\n",
              " 'america': 946,\n",
              " 'result': 947,\n",
              " 'crazy': 948,\n",
              " 'battle': 949,\n",
              " 'worked': 950,\n",
              " 'incredibly': 951,\n",
              " 'setting': 952,\n",
              " 'earlier': 953,\n",
              " 'background': 954,\n",
              " 'mess': 955,\n",
              " 'cop': 956,\n",
              " 'writers': 957,\n",
              " 'fire': 958,\n",
              " 'copy': 959,\n",
              " 'realize': 960,\n",
              " 'dumb': 961,\n",
              " 'unique': 962,\n",
              " 'powerful': 963,\n",
              " 'mark': 964,\n",
              " 'lee': 965,\n",
              " 'business': 966,\n",
              " 'rate': 967,\n",
              " 'older': 968,\n",
              " 'dramatic': 969,\n",
              " 'pay': 970,\n",
              " 'following': 971,\n",
              " 'girlfriend': 972,\n",
              " 'directors': 973,\n",
              " 'joke': 974,\n",
              " 'plenty': 975,\n",
              " 'directing': 976,\n",
              " 'various': 977,\n",
              " 'baby': 978,\n",
              " 'creepy': 979,\n",
              " 'development': 980,\n",
              " 'appear': 981,\n",
              " 'brings': 982,\n",
              " 'front': 983,\n",
              " 'dream': 984,\n",
              " 'ask': 985,\n",
              " 'water': 986,\n",
              " 'rich': 987,\n",
              " 'bill': 988,\n",
              " 'admit': 989,\n",
              " 'apart': 990,\n",
              " 'joe': 991,\n",
              " 'political': 992,\n",
              " 'fairly': 993,\n",
              " 'leading': 994,\n",
              " 'reasons': 995,\n",
              " 'spent': 996,\n",
              " 'portrayed': 997,\n",
              " 'telling': 998,\n",
              " 'cover': 999,\n",
              " 'outside': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK-ZUxYTABfn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "484aea43-223b-4b4b-b0b3-614eb4f06969"
      },
      "source": [
        "x_tokenized = tokenizer.texts_to_sequences(x)\n",
        "\n",
        "# Let's print s string before and after tokenization and examine the differences\n",
        "print(x[0])\n",
        "print(x_tokenized[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n",
            "[28, 5, 2, 80, 2103, 46, 1073, 13, 101, 148, 40, 308, 3185, 399, 475, 27, 3196, 34, 24, 204, 15, 12, 7, 622, 49, 597, 17, 69, 8, 8, 2, 87, 149, 13, 3242, 69, 43, 3185, 14, 93, 5399, 3, 1, 135, 5, 571, 61, 269, 9, 204, 37, 2, 662, 140, 1741, 69, 12, 7, 22, 4, 120, 16, 2, 7889, 2334, 39, 1, 12, 120, 2596, 55, 5912, 17, 5511, 6, 1480, 377, 39, 571, 93, 7, 3805, 9, 2, 361, 357, 5, 2, 662, 8, 8, 10, 7, 434, 3185, 15, 13, 7, 2, 1, 359, 6, 2, 1, 6814, 2539, 1065, 1, 10, 2712, 1422, 21, 1, 539, 33, 4637, 2469, 5, 2, 1209, 118, 30, 2, 7018, 26, 2971, 1, 3, 392, 1, 35, 1, 7, 22, 300, 21, 2, 4911, 7365, 539, 7, 345, 6, 107, 1, 1, 1, 1, 5051, 7890, 2454, 3, 52, 35, 1, 328, 1, 7366, 1, 3, 1, 1, 24, 111, 226, 244, 8, 8, 11, 59, 132, 2, 281, 1325, 5, 2, 120, 7, 694, 6, 2, 193, 13, 10, 270, 118, 80, 277, 590, 3025, 835, 181, 1321, 4162, 16, 2524, 1244, 835, 1444, 835, 888, 3185, 150, 955, 184, 2, 87, 399, 11, 124, 211, 3242, 69, 15, 35, 1638, 10, 14, 2240, 11, 414, 132, 11, 14, 1593, 16, 10, 19, 15, 11, 288, 52, 11, 1418, 4, 1281, 16, 3185, 3, 190, 1, 6, 2, 300, 2047, 5, 2151, 571, 22, 40, 571, 19, 7659, 7155, 5011, 1, 27, 2984, 42, 16, 4, 1, 6905, 1, 505, 21, 643, 3, 77, 244, 17, 10, 70, 7599, 652, 711, 6905, 110, 663, 83, 1209, 1, 694, 6, 66, 575, 5, 921, 2022, 39, 1209, 560, 148, 3185, 23, 201, 427, 3820, 17, 49, 7, 3315, 806, 1604, 44, 23, 68, 77, 9, 1229, 17, 126, 4104, 487]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyo3Tkhk6C37"
      },
      "source": [
        "### Padding Sequences\n",
        "\n",
        "Let's look at the lengths of the different tokenized reviews\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKBiqbpo6Bse",
        "outputId": "e24342c1-43a7-4c8a-d9ba-06836c90496b"
      },
      "source": [
        "print(\"Length of Review 1:\", len(x_tokenized[0]))\n",
        "print(\"Length of Review 10:\", len(x_tokenized[9]))\n",
        "print(\"Length of Review 1000:\", len(x_tokenized[999]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Review 1: 314\n",
            "Length of Review 10: 34\n",
            "Length of Review 1000: 620\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS9gdzlM6WHf"
      },
      "source": [
        "We can see that each tokenized review has different length. And since these tokenized reviews will be used as input to the model (which needs to be of a fixed shape), then clearly this won't work, an extra preprocessing step is required called `pad_sequences` available at `tf.keras.preprocessing.sequence`, this function will ensure that sequences are of same length by either clipping the sequence or padding it with zeros. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhHEFTcu5zNO",
        "outputId": "09af9b3c-b256-494d-e4a5-1a1b25e83467"
      },
      "source": [
        "x_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    x_tokenized, # The sequences that will be padded/clipped \n",
        "    maxlen=max_sequence, # The maximum length of the sequence using max_sequence that's defined earlier\n",
        "    padding='post', # Where we'll add zeros if sequence length is shorter that the maximum length, this will add zeros to the end of the sentence\n",
        ")\n",
        "\n",
        "# Let's print out the length of some padded sequences  \n",
        "print(\"Length of Review 1:\", len(x_padded[0]))\n",
        "print(\"Length of Review 10:\", len(x_padded[9]))\n",
        "print(\"Length of Review 1000:\", len(x_padded[999]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Review 1: 128\n",
            "Length of Review 10: 128\n",
            "Length of Review 1000: 128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UZmOlhq7ddb"
      },
      "source": [
        "### Preprocess Targets\n",
        "Now that the inputs are all setup, let's work on the targets. Specifically, let's change posative/negative into 1/0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0A2QfBJ7vQG",
        "outputId": "4fe66870-a59e-4cf4-f735-57186d3d3a71"
      },
      "source": [
        "y.replace({'positive':1,'negative':0}, inplace=True)\n",
        "y"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        1\n",
              "1        1\n",
              "2        1\n",
              "3        0\n",
              "4        1\n",
              "        ..\n",
              "49995    1\n",
              "49996    0\n",
              "49997    0\n",
              "49998    0\n",
              "49999    0\n",
              "Name: sentiment, Length: 50000, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrE5ewXc70Iq"
      },
      "source": [
        "### Train/Test Split\n",
        "Now that everything is setup, all we need to do is creating our training/testing split using Scikit Learn's `model_selection.train_test_split`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vn3maambKXIc",
        "outputId": "c7907a49-72ed-4ac9-b9e4-804e35f9f042"
      },
      "source": [
        "x_train, x_test, y_train, y_test = model_selection.train_test_split(x_padded, y, test_size = 0.05, random_state=42, stratify=y)\n",
        "\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((47500, 128), (47500,), (2500, 128), (2500,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcYKOsiqEQT8"
      },
      "source": [
        "### Create TF Data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1tIT1ESy4X4"
      },
      "source": [
        "def dataset_creator(x,y):\n",
        "    data=tf.data.Dataset.from_tensor_slices((x,y))\n",
        "    data=data.shuffle(50000)\n",
        "    data=data.batch(64)\n",
        "    data=data.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return data\n",
        "\n",
        "train_dataset = dataset_creator(x_train,y_train)\n",
        "test_dataset = dataset_creator(x_test,y_test)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9qprkVSbsqo"
      },
      "source": [
        "## Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svemz4Ok9MRm"
      },
      "source": [
        "### Model 1 - Fully Connected Neural Network\n",
        "\n",
        "We'll be training a fully connected network, since we know that Dense layers don't work well with sequence data, we can assume that this model will perform poorly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swaoT_h69WRH",
        "outputId": "5cf63388-cbee-4d0d-9b42-1f8cacb6d106"
      },
      "source": [
        "model_fcnn = tf.keras.Sequential([\n",
        "      Input([max_sequence]), # Input shape is equal to the padded sequences maximum length (i.e. max_sequence)\n",
        "      Dense(units=128,activation='relu'),\n",
        "      Dropout(0.3),\n",
        "      Dense(units=1,activation='sigmoid'),\n",
        "])\n",
        "\n",
        "model_fcnn.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 16,641\n",
            "Trainable params: 16,641\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6DFWeWn9gc1"
      },
      "source": [
        "model_fcnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1EjrK-w92E1",
        "outputId": "de709fc5-6e03-45cb-8dd6-cfb775b6b3e6"
      },
      "source": [
        "model_fcnn.fit(train_dataset, epochs=5, validation_data=test_dataset)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "743/743 [==============================] - 3s 3ms/step - loss: 187.7384 - accuracy: 0.4957 - val_loss: 3.1982 - val_accuracy: 0.5092\n",
            "Epoch 2/5\n",
            "743/743 [==============================] - 2s 2ms/step - loss: 2.5613 - accuracy: 0.5039 - val_loss: 1.4086 - val_accuracy: 0.4980\n",
            "Epoch 3/5\n",
            "743/743 [==============================] - 2s 2ms/step - loss: 1.2787 - accuracy: 0.5059 - val_loss: 1.1438 - val_accuracy: 0.5080\n",
            "Epoch 4/5\n",
            "743/743 [==============================] - 2s 2ms/step - loss: 1.0783 - accuracy: 0.4993 - val_loss: 1.0781 - val_accuracy: 0.4940\n",
            "Epoch 5/5\n",
            "743/743 [==============================] - 2s 2ms/step - loss: 0.8990 - accuracy: 0.4967 - val_loss: 0.8985 - val_accuracy: 0.4992\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7836308e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-Bk97v_--IC"
      },
      "source": [
        "An binary accuracy of 50% means that it's equal to a random classifier, meaning that our model isn't quite able to learn. Let's add another thing to the network that might help if perform better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COEYkFAlEbR8"
      },
      "source": [
        "### Model 2 - Fully Connected Neural Network with Embedding Layer\n",
        "\n",
        "Word Embeddings takes in words and converts them to a feature vector that can represent the word with mode data points than just a single number, Word Embeddings learn the relationship between different words and provide more information on the meaning of the word.\n",
        "\n",
        "You can read more about Word Embeddings here: https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n",
        "\n",
        "Keras offers an Embedding layer that can be used for neural networks on text data. It is a flexible layer that can be used in a variety of ways, such as:\n",
        "\n",
        "*   It can be used alone to learn a word embedding that can be saved and used in another model later.\n",
        "*   It can be used as part of a deep learning model where the embedding is learned along with the model itself.\n",
        "*   It can be used to load a pre-trained word embedding model, a type of transfer learning.\n",
        "\n",
        "\n",
        "The Embedding layer is defined as the first hidden layer of a network.\n",
        "\n",
        "It must specify 3 arguments:\n",
        "\n",
        "`input_dim`: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
        "\n",
        "`output_dim`: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
        "\n",
        "`input_length`: This is the length of input sequences, as you would define for any input layer of a Keras model.\n",
        "For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T9JxgINFL6P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e45726d-8417-48e5-a99d-838c55ac97a1"
      },
      "source": [
        "model_fcnn_with_embeddings = tf.keras.Sequential([\n",
        "      Input([max_sequence]), # Input shape is equal to the padded sequences maximum length (i.e. max_sequence). Alternatively, this can be defined as a part of the Embedding layer\n",
        "      Embedding(vocab_size+1, 100, mask_zero=True), # Embedding layer with input dim of vocab_size + 1 (to account for paddings)\n",
        "      Flatten(),\n",
        "      Dense(units=128, activation='relu'),\n",
        "      Dropout(0.3),\n",
        "      Dense(units=1,activation='sigmoid'),\n",
        "])\n",
        "\n",
        "model_fcnn_with_embeddings.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 128, 100)          800100    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               1638528   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,438,757\n",
            "Trainable params: 2,438,757\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLMIg5SjA-eJ"
      },
      "source": [
        "model_fcnn_with_embeddings.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fYaxTfgGNA6",
        "outputId": "51835118-3508-4e1e-ae60-205dbff50780"
      },
      "source": [
        "model_fcnn_with_embeddings.fit(train_dataset, epochs=5, validation_data=test_dataset)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "743/743 [==============================] - 8s 10ms/step - loss: 0.4712 - accuracy: 0.7458 - val_loss: 0.2980 - val_accuracy: 0.8768\n",
            "Epoch 2/5\n",
            "743/743 [==============================] - 7s 10ms/step - loss: 0.1206 - accuracy: 0.9576 - val_loss: 0.4106 - val_accuracy: 0.8532\n",
            "Epoch 3/5\n",
            "743/743 [==============================] - 8s 10ms/step - loss: 0.0196 - accuracy: 0.9951 - val_loss: 0.6525 - val_accuracy: 0.8488\n",
            "Epoch 4/5\n",
            "743/743 [==============================] - 8s 10ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.8219 - val_accuracy: 0.8556\n",
            "Epoch 5/5\n",
            "743/743 [==============================] - 8s 10ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.9095 - val_accuracy: 0.8464\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f78353fa8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGAA-3o7BTTo"
      },
      "source": [
        "The inclusion of the `Embedding` layer provided the network with better word representation that allowed the model to pick up on important features and use it to classify sentiment. But the performance can still be improved once the model is able to understand sequences. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEdOLDBjFVpH"
      },
      "source": [
        "### Model 3 - Recurrent Neural Network\n",
        "\n",
        "RNNs have an advantage over regular Dense layers, which is that they are able to hold on into sequential information during prediction, this allows RNNs to perform better than a Dense networks but it will require more training time due to the complexity of RNN layers compared to Dense layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpzdKn_fGlpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6176185-56f5-4274-fe29-e9dd25164460"
      },
      "source": [
        "model_rnn = tf.keras.Sequential([\n",
        "      Input([max_sequence]), # Input shape is equal to the padded sequences maximum length (i.e. max_sequence). Alternatively, this can be defined as a part of the Embedding layer\n",
        "      Embedding(vocab_size+1, 100, mask_zero=True,), # Embedding layer with input dim of vocab_size + 1 (to account for paddings)\n",
        "      SimpleRNN(128,return_sequences=True),\n",
        "      SimpleRNN(64),\n",
        "      Dense(units=1,activation='sigmoid'),                             \n",
        "])\n",
        "\n",
        "model_rnn.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 128, 100)          800100    \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       (None, 128, 128)          29312     \n",
            "_________________________________________________________________\n",
            "simple_rnn_1 (SimpleRNN)     (None, 64)                12352     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 841,829\n",
            "Trainable params: 841,829\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtMoD8i3IkC1"
      },
      "source": [
        "model_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlfHI2JlIur2",
        "outputId": "500d9a79-4835-4314-a450-b044211de472"
      },
      "source": [
        "model_rnn.fit(train_dataset, epochs=4, validation_data=test_dataset)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "743/743 [==============================] - 235s 314ms/step - loss: 0.6943 - accuracy: 0.5224 - val_loss: 0.6024 - val_accuracy: 0.6748\n",
            "Epoch 2/4\n",
            "743/743 [==============================] - 231s 311ms/step - loss: 0.5322 - accuracy: 0.7332 - val_loss: 0.5270 - val_accuracy: 0.7540\n",
            "Epoch 3/4\n",
            "743/743 [==============================] - 232s 312ms/step - loss: 0.5594 - accuracy: 0.7095 - val_loss: 0.7666 - val_accuracy: 0.7080\n",
            "Epoch 4/4\n",
            "743/743 [==============================] - 232s 312ms/step - loss: 0.4757 - accuracy: 0.7810 - val_loss: 0.5005 - val_accuracy: 0.7576\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7835128be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnZr6bFzDLHG"
      },
      "source": [
        "### Model 4 - Long Short Term Memory\n",
        "\n",
        "LSTMs has the advantage over RNNs in that they have gates that tells the cell which information to forget/hold onto instead of simply passing the data to the next cell. This allows the model to selectivly remember/forget things depending on their significance.\n",
        "\n",
        "Read more about RNNs and LSTMs here: https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM8JzXJZDLHH",
        "outputId": "99579dbe-aef4-47ea-9c89-369992884a68"
      },
      "source": [
        "model_lstm = tf.keras.Sequential([\n",
        "      Input([max_sequence]), # Input shape is equal to the padded sequences maximum length (i.e. max_sequence). Alternatively, this can be defined as a part of the Embedding layer\n",
        "      Embedding(vocab_size+1, 100, mask_zero=True), # Embedding layer with input dim of vocab_size + 1 (to account for paddings)\n",
        "      LSTM(128),\n",
        "      Dense(units=1,activation='sigmoid'),                             \n",
        "])\n",
        "\n",
        "model_lstm.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 128, 100)          800100    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               117248    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 917,477\n",
            "Trainable params: 917,477\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VmkHwYlDLHH"
      },
      "source": [
        "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWRcois4DLHH",
        "outputId": "438a072b-200b-40bf-8b07-45ae93485eea"
      },
      "source": [
        "model_lstm.fit(train_dataset, epochs=3, validation_data=test_dataset)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "743/743 [==============================] - 23s 23ms/step - loss: 0.4654 - accuracy: 0.7609 - val_loss: 0.3290 - val_accuracy: 0.8648\n",
            "Epoch 2/3\n",
            "743/743 [==============================] - 16s 21ms/step - loss: 0.2527 - accuracy: 0.9006 - val_loss: 0.2815 - val_accuracy: 0.8776\n",
            "Epoch 3/3\n",
            "743/743 [==============================] - 17s 23ms/step - loss: 0.1996 - accuracy: 0.9213 - val_loss: 0.3287 - val_accuracy: 0.8784\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f77c4317d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_eb0AZ0E2Tb"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uWfD3tpjByU"
      },
      "source": [
        "def predict(text):\n",
        "  \n",
        "  tokenized_texts = tokenizer.texts_to_sequences([text])\n",
        "  input = tf.keras.preprocessing.sequence.pad_sequences(tokenized_texts, maxlen=max_sequence, padding='post')\n",
        "\n",
        "  output = model_lstm.predict(input)[0][0]\n",
        "\n",
        "  print(\"The Sentence: \", text)\n",
        "\n",
        "  if output >= 0.5:\n",
        "    print(\"Is Postive\", output)\n",
        "  else:\n",
        "    print(\"Is Negative\", output)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8qjXmYHFCzQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d63d85-8c71-494e-d898-3543fe02e457"
      },
      "source": [
        "text = \"I am sad\"  #@param {type: \"string\"}\n",
        "\n",
        "predict(text)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Sentence:  I am sad\n",
            "Is Negative 0.1679133\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}